% Chapter 7

\chapter{Conclusions} % Main chapter title

\label{Chapter7} % For referencing the chapter elsewhere, use \ref{Chapter1} 

This thesis aimed to make a comparison between the model-based and the model-free approach in the context of Deep Reinforcement Learning (DRL). A set of tests have been executed over four environments from DeepMind Control Suite. The DRL algorithm known as Deep Deterministic Policy Gradient is used to test the model-free approach, while the chosen model-based algorithm was PlaNet.

The DDPG algorithm was implemented and tested over all the four environments chosen for the experiments. Some modifications, suggested by the Control Suite authors, were applied to the original algorithm and  parameters to adapt them to the Control Suite environments. After this improvement, the algorithm has been able to learn a policy when a state feature was provided.
These suggestions are not provided for the version with raw-pixels input. This problem is more difficult since the input is way more complicated. Features state is described by a vector of 18 dimensions (for the cheetah problem) while a single frame is a 64x64 RGB image. For the raw pixels input version the Control Suite authors have used a more advanced version of the algorithm called Distributed Distributional Deterministic Policy Gradients (D4PG). They showed that this version could also learn in this condition but is not capable of achieving the same performances of the experiments with features vector as input.
Moreover, that model has required $10^8$ number of samples. For our work, we still tried to train a DDPG model with frames as input, but we have been not enough computational budget to reach such a high number of steps, so we stop the training after 1000 episodes ($10^6$ steps, like all the other experiments). These results showed how difficult is the problem of solving that benchmark environment directly from raw-pixels input. 

PlaNet algorithm instead is natively designed to work with raw-pixels, and so  the algorithm converged for all the tested environments. We also discovered how to improve the general performance by removing the frame's compression in the preprocessing phase asking directly the Control Suite to render the frames in the specific dimensions required.

Next, two main ideas to improve the PlaNet model were tested. The first one was about to use the obtained reward as additional information to enrich the current state, but it failed.
The second idea is based on the fact that the model performances are directly connected to the reward predictions. In the early episodes, where the model is not trained, it tends to be too optimistic and to give erroneous information to the planner. This leads to a suboptimal plan and so a low cumulated reward, because the planner will exploit the weaknesses of the predictive model instead of optimizing the real agent's behaviour. For this reason, the second idea was to improve the model prediction ability by forcing the predictions to stay close to the collected experience. In other words, during the training, we incremented the model loss when the prediction was "unlikely" with respect to the trajectory collected in the experience replay buffer. The persistence of the regularizer can penalize the model because it limits the exploration of the environment, so we reduce the impact of the it during the training. In this way, we obtained a positive impact from the first iterations, and we maintained the same performance in the last episodes. We see a positive impact from the use of the regularizer, and we believe it deserves further study and experimentation, even with other environments.

The PlaNet model was able to reach better results with respect to the DDPG algorithm even if it worked directly with raw pixels while DDPG worked with feature states. This result is confirmed also for the other three environments and showed how the model-based approach leads to better performance and more sample efficiency. Since the network architecture in the model-based approach is more complex, the training time is longer. The DDPG model is faster at inference time and required less clock time to be trained (but more samples). For the task of the train an agent in the real world, the sample efficiency is a critic parameter. In facts, the cost of acquiring samples in a real environment is an order of magnitude greater than train the model with them.
Furthermore, the RGB camera is a very common, powerful and generic (not single task-specific) type of sensor that a lot of real-world robots could use. For these reasons, even if PlaNet algorithm has not already tested in a real-world scenario, we consider it a fundamental milestone to achieve the use of the Reinforcement Learning for a robot in the real world.

The following could be possible improvements for future research directions.\\

DDPG works well when a full markovian state is provided. We saw that the PlaNet model could produce a latent space that contains enough information to allow predictions over multiple steps. An interesting experiment could be to use PlaNet as an encoder to produce the latent states used then to train the DDPG algorithm. \\
%dreamer: https://arxiv.org/pdf/1912.01603.pdf

In the model-based approach, the planner ability is fixed and does not improve during the training epochs, as we saw with the model-free policy. The performance improvements are due to the increment of the knowledge about the environment that allows the model to make better predictions. So, to have a better model, we need to reduce the uncertainty over the environment. An idea of improvement could be the change of the planner objective in favour of the exploration during training. Once the model is fully trained a reward exploiting objective could be restored.
%plan to explore: https://arxiv.org/pdf/2005.05960.pdf

